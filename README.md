# Cloud Data Ingestion & Analytics Platform

Automa√ß√£o completa para coletar dados p√∫blicos brasileiros (Receita Federal, PGFN/Fazenda, Banco Central), armazenar em Google Cloud Storage, carregar no BigQuery e treinar modelos em notebooks Jupyter.

---

## üîé Vis√£o Geral

- **Ingest√£o:** Cloud Functions Gen2 acionadas por Pub/Sub fazem download, extra√ß√£o e upload organizado dos dados.
- **Orquestra√ß√£o:** Workflows + Cloud Scheduler permitem execu√ß√µes mensais, trimestrais ou sob demanda.
- **Curadoria:** Loaders dedicados (`BigQuery_loader_*`) movem os CSV/JSON do GCS para datasets particionados no BigQuery.
- **Analytics/ML:** Notebooks em `models/` usam o dataset *silver* para an√°lises explorat√≥rias e predi√ß√£o de `situacao_cadastral`, com vers√µes Pandas e Spark.

Consulte `ARCHITECTURE.md` para o fluxo ponta a ponta e `DEPLOY.md` para os comandos oficiais de implanta√ß√£o.

---

## üìÅ Estrutura Principal

| Caminho | Conte√∫do |
| --- | --- |
| `Cloud_Functions/` | Crawlers (Receita, Fazenda, Banco Central) e loaders BigQuery prontos para Pub/Sub |
| `BigQuery/` | Scripts standalone (`loader.py`, `loader_receita.py`) e documenta√ß√£o complementar |
| `models/` | Notebooks `ml_model_prediction_silver.ipynb`, vers√£o Spark e datasets CSV |
| `scripts/` | Automa√ß√£o: deploy dos loaders, workflow, schedulers, quickstart e envio em lote |
| `docs/` | Metadados oficiais (`cnpj-metadados.pdf`, dicion√°rios de campos) |
| `ARCHITECTURE.md` | Detalhes da arquitetura e fluxos |
| `DEPLOY.md` | Guia completo de deploy/update das Cloud Functions e loaders |
| `QUICKSTART.md` | Passo a passo em 5 minutos para subir o essencial |

---

## ‚öôÔ∏è Fluxo de Dados Resumido

1. **Cloud Scheduler** dispara o **Workflow** com um payload (`type`) ou voc√™ publica manualmente no Pub/Sub.
2. **Workflow** invoca a Cloud Function adequada (Receita/Fazenda/Banco Central); cada fun√ß√£o:
   - baixa o ZIP/JSON,
   - extrai/normaliza,
   - grava no bucket `gs://dados-cnpjs/<fonte>/<per√≠odo>/`,
   - cria marcadores `.extracted` para evitar reprocessos.
3. **Loaders** (`BigQuery_loader_fazenda_CF`, `BigQuery_loader_receita_CF`) movem os dados para tabelas no dataset `main_database`.
4. **Notebooks** consomem as camadas *silver* (CSV ou BigQuery) para an√°lises e modelos.

---

## üöÄ Guia R√°pido

```bash
# Autentica√ß√£o e projeto
gcloud auth login
gcloud config set project <SEU_PROJETO>

# APIs fundamentais
gcloud services enable cloudfunctions.googleapis.com cloudbuild.googleapis.com \
  pubsub.googleapis.com storage.googleapis.com bigquery.googleapis.com \
  workflows.googleapis.com cloudscheduler.googleapis.com

# Bucket padr√£o
gsutil mb -l southamerica-east1 gs://dados-cnpjs

# Deploy dos loaders BigQuery (Opcional: usa defaults do script)
./scripts/deploy-loaders.sh

# Deploy manual de um crawler (exemplo: Receita empresas)
gcloud functions deploy crawler-receita-empresas \
  --gen2 --runtime=python311 --region=us-east1 \
  --source=./Cloud_Functions/Receita_empresas_CF \
  --entry-point=crawler_receita_pubsub \
  --trigger-topic=receita-empresas-download \
  --timeout=540s --memory=8Gi --max-instances=1 \
  --set-env-vars DESTINATION_BUCKET_NAME=dados-cnpjs,BASE_PATH=receita_federal/empresas
```

Para comandos completos (incluindo Banco Central e o runner local dos loaders) veja `DEPLOY.md`. Se preferir um setup guiado, execute `./scripts/quickstart.sh`.

---

## üì¨ Opera√ß√µes Cotidianas

- **Executar ingest√£o manual:** publique no Pub/Sub correspondente (`receita-estabelecimentos-download`, `fazenda-download`, etc.).
- **Carregar no BigQuery localmente:** `python Cloud_Functions/BigQuery_loader_receita_CF/run_loader_empresas_local.py --data-type empresas --period 2024-03`.
- **Agendar execu√ß√µes:** utilize `scripts/setup-schedulers.sh` ou os comandos da se√ß√£o Schedulers em `DEPLOY.md`.
- **Monitorar:** `gcloud functions logs read <nome> --gen2 --region <regi√£o> --limit 100` e dashboards de workflow descritos em `ARCHITECTURE.md`.

---

## ü§ñ Machine Learning

- `ml_model_prediction_silver.ipynb`: fluxo completo com Pandas/Sklearn (split 70/20/10) para prever `situacao_cadastral`.
- `ml_model_prediction_silver_spark.ipynb`: feature engineering e splitting em PySpark, treinamento em Sklearn e salvamento em `models_pickle/`.
- Datasets: `dataset_metrics_silver.csv`, `dataset_silver.csv`.
- Rodar notebooks via VS Code/Jupyter local ou ambiente Dataproc/Spark, apontando para os CSVs no diret√≥rio `models/` ou BigQuery.

---

## üìö Documenta√ß√£o Complementar

- `ARCHITECTURE.md` ‚Äì diagramas e detalhes de seguran√ßa, custo, ciclo de vida.
- `DEPLOY.md` ‚Äì lista completa de comandos gcloud e payloads.
- `BigQuery/README.md` ‚Äì instru√ß√µes para uso dos scripts offline e schemas.
- `docs/` ‚Äì layouts oficiais (ex.: `cnpj-metadados.pdf` para o schema de empresas).

---

## ‚úÖ Checklist Antes do Deploy

- [ ] `gcloud config get-value project` mostra o projeto correto
- [ ] Bucket GCS criado e acess√≠vel (`gs://dados-cnpjs` ou equivalente)
- [ ] APIs habilitadas
- [ ] Vari√°veis de ambiente ajustadas nos comandos de deploy (bucket, base path, dataset)
- [ ] Se usar Workflows, arquivo `scripts/data-ingestion-workflow.yaml` dispon√≠vel

# Cloud Data Platform ‚Äì Ingest√£o + BigQuery + ML

Pipelines completos para coletar, organizar e analisar dados p√∫blicos brasileiros com Cloud Functions, BigQuery loaders e notebooks de Machine Learning.

## Fast Track

1. **Configurar GCP** ‚Äì `gcloud auth login`, `gcloud config set project <id>`  
2. **Provisionar infraestrutura** ‚Äì buckets + APIs (ver `DEPLOY.md`)  
3. **Deploy** ‚Äì use os comandos da se√ß√£o *Cloud Functions* abaixo ou o script `scripts/deploy-loaders.sh` para os loaders do BigQuery  
4. **Acionar** ‚Äì publique mensagens nas filas Pub/Sub indicadas ou execute `Cloud_Functions/BigQuery_loader_receita_CF/run_loader_empresas_local.py` para rodar localmente  
5. **Analisar** ‚Äì rode os notebooks em `models/` para explorar e treinar modelos

> Precisa de um passo a passo guiado? Consulte `QUICKSTART.md`.

## Reposit√≥rio em um olhar

| Diret√≥rio/Arquivo | Descri√ß√£o |
| --- | --- |
| `Cloud_Functions/` | Fun√ß√µes de ingest√£o (Receita, Fazenda, Banco Central) e loaders BigQuery |
| `BigQuery/` | Scripts standalone e instru√ß√µes para cargas diretas a partir do GCS |
| `models/` | Notebooks e scripts de modelagem (`ml_model_prediction_silver.ipynb`, vers√£o Spark, etc.) |
| `scripts/` | Automa√ß√£o de deploy, schedulers e envio em lote para Pub/Sub |
| `docs/` | Metadados oficiais (schemas Receita, dicion√°rios) |
| `ARCHITECTURE.md` | Vis√£o ponta a ponta (Scheduler ‚Üí Workflow ‚Üí CF ‚Üí GCS ‚Üí BigQuery) |
| `DEPLOY.md` | Guia √∫nico de deploy e opera√ß√£o |
| `QUICKSTART.md` | Deploy resumido (5 minutos) |

## Cloud Functions & Loaders

| Fonte | Fun√ß√£o (`Cloud_Functions/<dir>`) | Trigger/Script | Observa√ß√µes |
| --- | --- | --- | --- |
| Receita ‚Äì Estabelecimentos | `Receita_estabelecimentos_CF` | Pub/Sub `receita-estabelecimentos-download` | Processa ZIP ‚Üí CSV no GCS |
| Receita ‚Äì Empresas | `Receita_empresas_CF` | Pub/Sub `receita-empresas-download` | Mesmo fluxo com arquivos `EMPRECSV` |
| Receita ‚Äì Lucros | `Receita_lucros_CF` | Pub/Sub `receita-lucros-download` | Mant√©m os 4 regimes separados |
| PGFN (Fazenda) | `Fazenda_CF` | Pub/Sub `fazenda-download` | Baixa os 3 blocos (FGTS, Previd., N√£o Prev.) |
| Banco Central | `Banco_Central_CF` | Pub/Sub `banco-central-download` | Agrega indicadores macro |
| Loader PGFN ‚Üí BigQuery | `BigQuery_loader_fazenda_CF` | Pub/Sub `bigquery-loader-fazenda` ou `scripts/deploy-loaders.sh` | Escreve em `pgfn_*` (bronze/silver) |
| Loader Receita ‚Üí BigQuery | `BigQuery_loader_receita_CF` | Pub/Sub `bigquery-loader-receita` ou runner local | Carrega Estabelecimentos + Empresas; suporta `data_type` e `period` no payload |

### Executar loaders localmente (evitar timeout do Cloud Run)

```bash
cd Cloud_Functions/BigQuery_loader_receita_CF
python run_loader_empresas_local.py --data-type empresas --period 2024-03
# ou para todos os per√≠odos
python run_loader_empresas_local.py --data-type all --mode all
```

### Payload padr√£o Pub/Sub (loader Receita ‚Üí BigQuery)

```json
{
  "period": "2024-03",          // opcional
  "data_type": "empresas",      // "estabelecimentos" | "empresas" | "all"
  "write_mode": "WRITE_APPEND"  // ou WRITE_TRUNCATE
}
```

Detalhes sobre schemas, modos de escrita e estrat√©gias de custo: veja `BigQuery/README.md`.

## Automa√ß√£o & Orquestra√ß√£o

- **`scripts/deploy-loaders.sh`** ‚Äì Deploy end-to-end dos loaders BigQuery (cria t√≥picos Pub/Sub, habilita APIs, sobe as fun√ß√µes Gen2).  
- **`scripts/enviar_mensagens_lote.sh`** ‚Äì Publica mensagens para recuperar m√∫ltiplos per√≠odos de uma vez.  
- **`scripts/setup-schedulers.sh`** ‚Äì Cria Cloud Schedulers alinhados ao calend√°rio definido em `scripts/README.md`.  
- **Workflows/Schedulers** ‚Äì Toda a l√≥gica (tipo de execu√ß√£o, hor√°rios, troubleshooting) est√° documentada em `scripts/README.md`.

## Machine Learning (models/)

- `ml_model_prediction_silver.ipynb` ‚Äì Modelo tradicional em Pandas/Sklearn (treino/val/test 70/20/10).  
- `ml_model_prediction_silver_spark.ipynb` ‚Äì Vers√£o Spark-first: feature engineering em PySpark, split temporal em Spark, convers√£o controlada para Pandas apenas no momento do treinamento. Salva modelo + mapeamentos em `models_pickle/`.  
- `ml_model_analysis.ipynb` ‚Äì Notebook base explorat√≥rio.  
- `dataset_metrics_silver.csv` ‚Äì Base consolidada (Silver) usada nos notebooks.

## Principais comandos de opera√ß√£o

```bash
# Receita (download) - processar arquivo espec√≠fico
gcloud pubsub topics publish receita-estabelecimentos-download \
  --message='{"folder": "2024-03", "file": "Estabelecimentos0.zip"}'

# Loader Receita ‚Üí BigQuery - per√≠odo √∫nico
gcloud pubsub topics publish bigquery-loader-receita \
  --message='{"period": "2024-03", "data_type": "empresas", "write_mode": "WRITE_APPEND"}'

# PGFN download completo
gcloud pubsub topics publish fazenda-download --message='{}'
```

Para logs, agendamentos e remo√ß√£o de recursos, siga `DEPLOY.md` (cobre comandos `gcloud functions logs read`, schedulers e limpeza).

## Documentos complementares

- `DEPLOY.md` ‚Äì comandos completos de deploy/atualiza√ß√£o, payloads e configura√ß√£o de vari√°veis de ambiente.  
- `ARCHITECTURE.md` ‚Äì diagrama + fluxo detalhado Scheduler ‚Üí Workflow ‚Üí Cloud Functions ‚Üí GCS ‚Üí BigQuery ‚Üí notebooks.  
- `BigQuery/README.md` ‚Äì instru√ß√µes para cargas offline, schemas e troubleshooting.  
- `scripts/README.md` ‚Äì orquestra√ß√£o via Workflows + Cloud Scheduler.  
- `docs/*.pdf|xlsx` ‚Äì metadados oficiais (ex.: `cnpj-metadados.pdf` para o schema de empresas).

---

Com isso voc√™ tem ingest√£o automatizada por Pub/Sub, cargas confi√°veis para BigQuery e notebooks prontos para modelagem. Bons experimentos! üöÄ

