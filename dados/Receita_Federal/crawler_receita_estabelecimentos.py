#!/usr/bin/env python3
"""
Script para baixar todos os arquivos de Estabelecimentos da Receita Federal
Baixa de todos os anos/meses disponÃ­veis no servidor
"""

import os
import re
import sys
import time
import zipfile
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import wget


# =============================================================================
# CONFIGURAÃ‡Ã•ES
# =============================================================================
BASE_URL = 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/'
DOWNLOAD_DIR = Path(__file__).parent / 'downloads'
EXTRACTED_DIR = Path(__file__).parent / 'extracted'
MAX_RETRIES = 3
TIMEOUT = 60

# Filtro de perÃ­odo (formato YYYY-MM)
# NOTA: O servidor sÃ³ tem dados a partir de 2023-05
START_YEAR_MONTH = '2020-01'  # InÃ­cio do perÃ­odo desejado (tentarÃ¡ desde 2020)
END_YEAR_MONTH = '2025-12'    # Fim do perÃ­odo desejado


# =============================================================================
# FUNÃ‡Ã•ES AUXILIARES
# =============================================================================

def make_dirs():
    """Cria os diretÃ³rios necessÃ¡rios"""
    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)
    EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)


def make_request_with_retry(url, max_retries=MAX_RETRIES, timeout=TIMEOUT):
    """Faz requisiÃ§Ã£o HTTP com retry automÃ¡tico"""
    retry_delay = 10
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f'   Tentativa {attempt + 1}/{max_retries}...')
            
            response = requests.get(url, timeout=timeout, allow_redirects=True)
            response.raise_for_status()
            return response
            
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f'   âš ï¸  Timeout. Aguardando {retry_delay}s...')
                time.sleep(retry_delay)
            else:
                raise
                
        except requests.exceptions.ConnectionError:
            if attempt < max_retries - 1:
                print(f'   âš ï¸  Erro de conexÃ£o. Aguardando {retry_delay}s...')
                time.sleep(retry_delay)
            else:
                raise
                
        except Exception:
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
            else:
                raise
    
    raise Exception('MÃ¡ximo de tentativas atingido')


def get_available_folders(base_url):
    """
    Lista todas as pastas ano-mÃªs disponÃ­veis no servidor
    Retorna lista ordenada (mais antiga primeiro), filtrada pelo perÃ­odo configurado
    """
    print(f'ðŸ” Buscando pastas disponÃ­veis em: {base_url}')
    print(f'   PerÃ­odo desejado: {START_YEAR_MONTH} atÃ© {END_YEAR_MONTH}')
    
    try:
        response = make_request_with_retry(base_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Procurar por links que parecem pastas de data (YYYY-MM/)
        folders = []
        for link in soup.find_all('a'):
            href = link.get('href', '')
            # PadrÃ£o: YYYY-MM/ (ex: 2024-05/)
            if re.match(r'^\d{4}-\d{2}/$', href):
                # Remover a barra final para comparaÃ§Ã£o
                folder_date = href.rstrip('/')
                
                # Filtrar pelo perÃ­odo configurado
                if START_YEAR_MONTH <= folder_date <= END_YEAR_MONTH:
                    folders.append(href)
        
        # Ordenar cronologicamente (mais antiga primeiro)
        folders.sort()
        
        if folders:
            print(f'âœ“ Encontradas {len(folders)} pastas no perÃ­odo')
            print(f'  Intervalo disponÃ­vel: {folders[0]} atÃ© {folders[-1]}')
        else:
            print(f'âš ï¸  Nenhuma pasta encontrada no perÃ­odo {START_YEAR_MONTH} a {END_YEAR_MONTH}!')
            print('   O servidor pode nÃ£o ter dados desse perÃ­odo disponÃ­veis.')
        
        return folders
        
    except Exception as e:
        print(f'âŒ Erro ao listar pastas: {e}')
        return []


def get_estabelecimento_files(folder_url):
    """
    Lista todos os arquivos de Estabelecimentos de uma pasta especÃ­fica
    Retorna lista de nomes de arquivos
    """
    try:
        response = make_request_with_retry(folder_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Procurar por arquivos .zip que contenham "Estabelecimento" + nÃºmero
        # PadrÃ£o: Estabelecimentos0.zip, Estabelecimentos1.zip, etc.
        files = []
        pattern = re.compile(r'Estabelecimentos?\d+\.zip', re.IGNORECASE)
        
        for link in soup.find_all('a'):
            href = link.get('href', '')
            if pattern.match(href):
                files.append(href)
        
        files.sort()
        return files
        
    except Exception as e:
        print(f'   âŒ Erro ao listar arquivos: {e}')
        return []


def download_file(url, dest_path):
    """
    Baixa um arquivo se ele nÃ£o existir ou estiver corrompido
    Retorna True se baixou/jÃ¡ existia, False se falhou
    """
    file_name = dest_path.name
    
    # Verificar se jÃ¡ existe e estÃ¡ OK
    if dest_path.exists():
        try:
            # Tentar abrir o zip para verificar integridade
            with zipfile.ZipFile(dest_path, 'r') as zip_ref:
                if zip_ref.testzip() is None:
                    print(f'   âœ“ Arquivo jÃ¡ existe e estÃ¡ Ã­ntegro')
                    return True
        except zipfile.BadZipFile:
            print(f'   âš ï¸  Arquivo corrompido, baixando novamente...')
            dest_path.unlink()
    
    # Baixar arquivo
    try:
        print(f'   â¬‡ï¸  Baixando...')
        
        # Usar wget para download com barra de progresso
        wget.download(url, out=str(dest_path.parent), bar=bar_progress)
        print()  # Nova linha apÃ³s o progresso
        
        # Verificar integridade do arquivo baixado
        with zipfile.ZipFile(dest_path, 'r') as zip_ref:
            if zip_ref.testzip() is not None:
                print(f'   âŒ Arquivo baixado estÃ¡ corrompido!')
                dest_path.unlink()
                return False
        
        print(f'   âœ… Download concluÃ­do com sucesso')
        return True
        
    except Exception as e:
        print(f'   âŒ Erro no download: {str(e)[:100]}')
        if dest_path.exists():
            dest_path.unlink()
        return False


def extract_file(zip_path, extract_dir):
    """
    Extrai um arquivo zip
    Retorna True se extraiu com sucesso, False se falhou
    """
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # Verificar integridade
            if zip_ref.testzip() is not None:
                print(f'   âŒ Arquivo corrompido')
                return False
            
            # Verificar se jÃ¡ foi extraÃ­do
            members = zip_ref.namelist()
            already_extracted = all(
                (extract_dir / m).exists() 
                for m in members if not m.endswith('/')
            )
            
            if already_extracted:
                print(f'   âœ“ JÃ¡ extraÃ­do')
                return True
            
            # Extrair
            print(f'   ðŸ“¦ Extraindo...')
            zip_ref.extractall(extract_dir)
            print(f'   âœ… ExtraÃ­do com sucesso')
            return True
            
    except Exception as e:
        print(f'   âŒ Erro na extraÃ§Ã£o: {str(e)[:100]}')
        return False


def bar_progress(current, total, width=80):
    """Barra de progresso para wget"""
    progress_message = f"   {current / total * 100:.1f}% [{current:,} / {total:,}] bytes"
    sys.stdout.write("\r" + progress_message)
    sys.stdout.flush()


def format_size(bytes_size):
    """Formata tamanho em bytes para formato legÃ­vel"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f} TB"


def format_time(seconds):
    """Formata tempo em segundos para formato legÃ­vel"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}min"
    else:
        return f"{seconds/3600:.1f}h"


# =============================================================================
# FUNÃ‡ÃƒO PRINCIPAL
# =============================================================================

def main():
    """FunÃ§Ã£o principal que coordena o download"""
    print('=' * 80)
    print('CRAWLER - ESTABELECIMENTOS RECEITA FEDERAL')
    print('=' * 80)
    print()
    
    start_time = time.time()
    
    # Criar diretÃ³rios
    make_dirs()
    
    # Listar todas as pastas disponÃ­veis
    folders = get_available_folders(BASE_URL)
    
    if not folders:
        print('âŒ Nenhuma pasta encontrada. Encerrando.')
        sys.exit(1)
    
    print()
    print(f'ðŸ“‹ SerÃ£o processadas {len(folders)} pastas')
    print()
    
    # EstatÃ­sticas
    total_files = 0
    downloaded_files = 0
    skipped_files = 0
    failed_files = 0
    extracted_files = 0
    
    # Processar cada pasta
    for folder_idx, folder in enumerate(folders, 1):
        folder_url = urljoin(BASE_URL, folder)
        folder_name = folder.rstrip('/')
        
        print('-' * 80)
        print(f'[{folder_idx}/{len(folders)}] Processando pasta: {folder_name}')
        print('-' * 80)
        
        # Criar subdiretÃ³rio para esta pasta
        folder_download_dir = DOWNLOAD_DIR / folder_name
        folder_extract_dir = EXTRACTED_DIR / folder_name
        folder_download_dir.mkdir(parents=True, exist_ok=True)
        folder_extract_dir.mkdir(parents=True, exist_ok=True)
        
        # Listar arquivos de estabelecimentos
        files = get_estabelecimento_files(folder_url)
        
        if not files:
            print(f'âš ï¸  Nenhum arquivo de Estabelecimentos encontrado')
            print()
            continue
        
        print(f'ðŸ“¦ Encontrados {len(files)} arquivos de Estabelecimentos')
        print()
        
        # Baixar cada arquivo
        for file_idx, file_name in enumerate(files, 1):
            total_files += 1
            file_url = urljoin(folder_url, file_name)
            dest_path = folder_download_dir / file_name
            
            print(f'  [{file_idx}/{len(files)}] {file_name}')
            
            # Verificar se jÃ¡ existe
            if dest_path.exists():
                try:
                    with zipfile.ZipFile(dest_path, 'r') as zip_ref:
                        if zip_ref.testzip() is None:
                            print(f'   âœ“ JÃ¡ baixado e Ã­ntegro')
                            skipped_files += 1
                            
                            # Tentar extrair
                            if extract_file(dest_path, folder_extract_dir):
                                extracted_files += 1
                            
                            print()
                            continue
                except:
                    pass
            
            # Baixar arquivo
            if download_file(file_url, dest_path):
                downloaded_files += 1
                
                # Extrair arquivo
                if extract_file(dest_path, folder_extract_dir):
                    extracted_files += 1
            else:
                failed_files += 1
            
            print()
        
        print()
    
    # Resumo final
    elapsed_time = time.time() - start_time
    
    print('=' * 80)
    print('RESUMO FINAL')
    print('=' * 80)
    print(f'ðŸ“Š EstatÃ­sticas:')
    print(f'   Total de arquivos processados: {total_files}')
    print(f'   âœ… Baixados:                   {downloaded_files}')
    print(f'   â­ï¸  Pulados (jÃ¡ existiam):      {skipped_files}')
    print(f'   âŒ Falhas:                      {failed_files}')
    print(f'   ðŸ“¦ ExtraÃ­dos:                   {extracted_files}')
    print()
    print(f'â±ï¸  Tempo total: {format_time(elapsed_time)}')
    print()
    print(f'ðŸ“ Arquivos salvos em: {DOWNLOAD_DIR}')
    print(f'ðŸ“ ExtraÃ­dos em:       {EXTRACTED_DIR}')
    print()
    
    # Calcular espaÃ§o usado
    total_size = sum(
        f.stat().st_size 
        for f in DOWNLOAD_DIR.rglob('*') 
        if f.is_file()
    )
    print(f'ðŸ’¾ EspaÃ§o utilizado: {format_size(total_size)}')
    print()
    print('âœ… Processo concluÃ­do!')


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print()
        print('âš ï¸  Processo interrompido pelo usuÃ¡rio')
        sys.exit(1)
    except Exception as e:
        print()
        print(f'âŒ Erro fatal: {e}')
        sys.exit(1)

