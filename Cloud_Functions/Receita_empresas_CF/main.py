#!/usr/bin/env python3
"""
Cloud Function para baixar arquivos de Empresas da Receita Federal
Salva diretamente no Google Cloud Storage
Baixa, extrai e deleta ZIPs automaticamente
"""

import os
import re
import io
import zipfile
import json
import base64
from pathlib import Path
from urllib.parse import urljoin
from typing import List, Dict, Tuple

import requests
from bs4 import BeautifulSoup
from google.cloud import storage
import functions_framework


# =============================================================================
# CONFIGURA√á√ïES
# =============================================================================
BASE_URL = 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/'
DESTINATION_BUCKET_NAME = os.environ.get('DESTINATION_BUCKET_NAME', 'seu-bucket-aqui')
BASE_PATH = os.environ.get('BASE_PATH', 'receita_federal')  # Caminho base no bucket

# Filtros de per√≠odo
START_YEAR_MONTH = os.environ.get('START_YEAR_MONTH', '2020-01')
END_YEAR_MONTH = os.environ.get('END_YEAR_MONTH', '2025-12')
ALLOWED_MONTHS = os.environ.get('ALLOWED_MONTHS', '').split(',')  # Vazio = todos meses

# Configura√ß√µes de download
MAX_RETRIES = 3
TIMEOUT = (30, 500)  # (connect timeout, read timeout)
CHUNK_SIZE = 1048576

# Inicializar cliente do Storage
storage_client = storage.Client()
bucket = storage_client.bucket(DESTINATION_BUCKET_NAME)


# =============================================================================
# FUN√á√ïES AUXILIARES
# =============================================================================

def make_request_with_retry(url: str, max_retries: int = MAX_RETRIES) -> requests.Response:
    """Faz requisi√ß√£o HTTP com retry autom√°tico"""
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f'   Tentativa {attempt + 1}/{max_retries}...')
            
            response = requests.get(url, timeout=TIMEOUT, allow_redirects=True)
            response.raise_for_status()
            return response
            
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f'   ‚ö†Ô∏è  Timeout. Tentando novamente...')
            else:
                raise
                
        except requests.exceptions.RequestException:
            if attempt < max_retries - 1:
                print(f'   ‚ö†Ô∏è  Erro de conex√£o. Tentando novamente...')
            else:
                raise
    
    raise Exception('M√°ximo de tentativas atingido')


def get_available_folders(base_url: str) -> List[str]:
    """
    Lista todas as pastas ano-m√™s dispon√≠veis no servidor
    Retorna lista ordenada, filtrada pelo per√≠odo e meses configurados
    """
    print(f'üîç Buscando pastas dispon√≠veis em: {base_url}')
    print(f'   Per√≠odo: {START_YEAR_MONTH} at√© {END_YEAR_MONTH}')
    if ALLOWED_MONTHS and ALLOWED_MONTHS != ['']:
        print(f'   Meses filtrados: {", ".join(ALLOWED_MONTHS)}')
    
    try:
        response = make_request_with_retry(base_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        folders = []
        for link in soup.find_all('a'):
            href = link.get('href', '')
            # Padr√£o: YYYY-MM/
            if re.match(r'^\d{4}-\d{2}/$', href):
                folder_date = href.rstrip('/')
                
                # Filtrar pelo per√≠odo
                if START_YEAR_MONTH <= folder_date <= END_YEAR_MONTH:
                    # Filtrar por meses espec√≠ficos
                    if ALLOWED_MONTHS and ALLOWED_MONTHS != ['']:
                        month = folder_date.split('-')[1]
                        if month in ALLOWED_MONTHS:
                            folders.append(href)
                    else:
                        folders.append(href)
        
        folders.sort()
        
        if folders:
            print(f'‚úì Encontradas {len(folders)} pastas')
        else:
            print(f'‚ö†Ô∏è  Nenhuma pasta encontrada')
        
        return folders
        
    except Exception as e:
        print(f'‚ùå Erro ao listar pastas: {e}')
        return []


def get_empresas_files(folder_url: str) -> List[str]:
    """Lista todos os arquivos de Empresas de uma pasta"""
    try:
        response = make_request_with_retry(folder_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        files = []
        pattern = re.compile(r'Empresas?\d+\.zip', re.IGNORECASE)
        
        for link in soup.find_all('a'):
            href = link.get('href', '')
            if pattern.match(href):
                files.append(href)
        
        files.sort()
        return files
        
    except Exception as e:
        print(f'   ‚ùå Erro ao listar arquivos: {e}')
        return []


def blob_exists(blob_path: str) -> bool:
    """Verifica se um blob existe no bucket"""
    blob = bucket.blob(blob_path)
    return blob.exists()


def check_extraction_marker(folder_name: str, zip_name: str) -> bool:
    """Verifica se existe marcador de extra√ß√£o para um ZIP"""
    marker_path = f'{BASE_PATH}/{folder_name}/.{Path(zip_name).stem}.extracted'
    return blob_exists(marker_path)


def create_extraction_marker(folder_name: str, zip_name: str):
    """Cria marcador de extra√ß√£o no bucket"""
    marker_path = f'{BASE_PATH}/{folder_name}/.{Path(zip_name).stem}.extracted'
    blob = bucket.blob(marker_path)
    blob.upload_from_string('extracted', content_type='text/plain')
    print(f'   ‚úì Marcador criado: {marker_path}')


def download_and_extract_to_gcs(url: str, folder_name: str, file_name: str) -> Tuple[bool, bool]:
    """
    Baixa ZIP, extrai conte√∫do para GCS e deleta ZIP (em mem√≥ria)
    Retorna: (download_success, extraction_success)
    """
    try:
        # Verificar se j√° foi extra√≠do
        if check_extraction_marker(folder_name, file_name):
            print(f'   ‚è≠Ô∏è  {file_name}: J√° extra√≠do, pulando...')
            return (True, True)
        
        print(f'   ‚¨áÔ∏è  {file_name}: Baixando...')
        
        # Download do arquivo ZIP em mem√≥ria
        response = requests.get(url, stream=True, timeout=TIMEOUT)
        response.raise_for_status()
        
        # Ler conte√∫do do ZIP em mem√≥ria
        zip_content = io.BytesIO()
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):
            if chunk:
                zip_content.write(chunk)
                downloaded += len(chunk)
                if total_size > 0:
                    percent = (downloaded / total_size) * 100
                    if downloaded % (CHUNK_SIZE * 100) == 0:  # Log a cada ~800KB
                        print(f'   {file_name}: {percent:.1f}%')
        
        print(f'   ‚úÖ {file_name}: Download conclu√≠do ({downloaded / 1024 / 1024:.1f} MB)')
        
        # Extrair e fazer upload dos arquivos
        print(f'   üì¶ {file_name}: Extraindo e enviando para GCS...')
        zip_content.seek(0)
        
        with zipfile.ZipFile(zip_content, 'r') as zip_ref:
            # Testar integridade
            if zip_ref.testzip() is not None:
                print(f'   ‚ùå {file_name}: ZIP corrompido')
                return (True, False)
            
            # Extrair cada arquivo
            members = zip_ref.namelist()
            files_uploaded = 0
            
            for member in members:
                if not member.endswith('/'):  # Ignorar diret√≥rios
                    # Ler arquivo do ZIP
                    file_data = zip_ref.read(member)
                    
                    # Caminho no bucket: BASE_PATH/folder_name/arquivo.csv
                    blob_path = f'{BASE_PATH}/{folder_name}/{member}'
                    blob = bucket.blob(blob_path)
                    
                    # Upload para GCS
                    blob.upload_from_string(file_data, content_type='text/csv')
                    files_uploaded += 1
                    
                    if files_uploaded % 5 == 0:
                        print(f'   ... {files_uploaded}/{len(members)} arquivos enviados')
            
            print(f'   ‚úÖ {file_name}: {files_uploaded} arquivos extra√≠dos para GCS')
            
            # Criar marcador de extra√ß√£o
            create_extraction_marker(folder_name, file_name)
        
        # ZIP √© automaticamente deletado (estava em mem√≥ria)
        print(f'   üóëÔ∏è  {file_name}: ZIP removido da mem√≥ria')
        
        return (True, True)
        
    except requests.exceptions.Timeout:
        print(f'   ‚ùå {file_name}: Timeout no download')
        return (False, False)
        
    except zipfile.BadZipFile:
        print(f'   ‚ùå {file_name}: Arquivo ZIP inv√°lido')
        return (True, False)
        
    except Exception as e:
        print(f'   ‚ùå {file_name}: Erro - {str(e)[:100]}')
        return (False, False)


def process_single_file(folder_name: str, file_name: str) -> Dict[str, any]:
    """
    Processa um √∫nico arquivo ZIP
    Retorna resultado do processamento
    """
    # Normalizar folder_name
    if not folder_name.endswith('/'):
        folder_name = folder_name + '/'
    
    folder_url = urljoin(BASE_URL, folder_name)
    file_url = urljoin(folder_url, file_name)
    
    print(f'\n{"=" * 80}')
    print(f'üìÅ Pasta: {folder_name.rstrip("/")}')
    print(f'üì¶ Arquivo: {file_name}')
    print(f'{"=" * 80}')
    
    download_ok, extract_ok = download_and_extract_to_gcs(file_url, folder_name.rstrip('/'), file_name)
    
    result = {
        'folder': folder_name.rstrip('/'),
        'file': file_name,
        'download_success': download_ok,
        'extraction_success': extract_ok
    }
    
    if download_ok and extract_ok:
        print(f'\n‚úÖ {file_name} processado com sucesso!')
    else:
        print(f'\n‚ùå {file_name} falhou!')
    
    return result


def list_files_in_folder(folder: str) -> List[str]:
    """
    Lista todos os arquivos de Empresas de uma pasta
    Retorna apenas os nomes dos arquivos
    """
    folder_url = urljoin(BASE_URL, folder)
    files = get_empresas_files(folder_url)
    return files


def process_folder(folder: str) -> Dict[str, int]:
    """
    Processa uma pasta espec√≠fica (baixa todos as empresas)
    Retorna estat√≠sticas
    """
    folder_url = urljoin(BASE_URL, folder)
    folder_name = folder.rstrip('/')
    
    print(f'\n{"=" * 80}')
    print(f'üìÅ Processando: {folder_name}')
    print(f'{"=" * 80}')
    
    # Listar arquivos
    files = get_empresas_files(folder_url)
    
    if not files:
        print(f'‚ö†Ô∏è  Nenhum arquivo encontrado')
        return {'total': 0, 'downloaded': 0, 'extracted': 0, 'skipped': 0, 'failed': 0}
    
    print(f'üì¶ Encontrados {len(files)} arquivos')
    
    stats = {
        'total': len(files),
        'downloaded': 0,
        'extracted': 0,
        'skipped': 0,
        'failed': 0
    }
    
    # Processar cada arquivo
    for idx, file_name in enumerate(files, 1):
        print(f'\n[{idx}/{len(files)}] {file_name}')
        
        file_url = urljoin(folder_url, file_name)
        download_ok, extract_ok = download_and_extract_to_gcs(file_url, folder_name, file_name)
        
        if download_ok and extract_ok:
            if check_extraction_marker(folder_name, file_name):
                stats['skipped'] += 1
            else:
                stats['downloaded'] += 1
                stats['extracted'] += 1
        elif download_ok and not extract_ok:
            stats['failed'] += 1
        else:
            stats['failed'] += 1
    
    return stats


# =============================================================================
# CLOUD FUNCTION HANDLERS
# =============================================================================

@functions_framework.http
def crawler_receita_http(request):
    """
    Handler HTTP - processa todas as pastas configuradas
    """
    print('=' * 80)
    print('CRAWLER RECEITA FEDERAL - CLOUD FUNCTION')
    print('=' * 80)
    print(f'Bucket destino: {DESTINATION_BUCKET_NAME}')
    print(f'Caminho base: {BASE_PATH}')
    print()
    
    # Listar pastas dispon√≠veis
    folders = get_available_folders(BASE_URL)
    
    if not folders:
        return {'error': 'Nenhuma pasta encontrada'}, 404
    
    print(f'\nüìã Total de pastas: {len(folders)}')
    
    # Estat√≠sticas globais
    global_stats = {
        'folders_processed': 0,
        'total_files': 0,
        'downloaded': 0,
        'extracted': 0,
        'skipped': 0,
        'failed': 0
    }
    
    # Processar cada pasta
    for folder in folders:
        stats = process_folder(folder)
        global_stats['folders_processed'] += 1
        global_stats['total_files'] += stats['total']
        global_stats['downloaded'] += stats['downloaded']
        global_stats['extracted'] += stats['extracted']
        global_stats['skipped'] += stats['skipped']
        global_stats['failed'] += stats['failed']
    
    # Resumo final
    print('\n' + '=' * 80)
    print('RESUMO FINAL')
    print('=' * 80)
    print(f"üìä Estat√≠sticas:")
    print(f"   Pastas processadas: {global_stats['folders_processed']}")
    print(f"   Total de arquivos:  {global_stats['total_files']}")
    print(f"   ‚úÖ Baixados:         {global_stats['downloaded']}")
    print(f"   üì¶ Extra√≠dos:        {global_stats['extracted']}")
    print(f"   ‚è≠Ô∏è  Pulados:          {global_stats['skipped']}")
    print(f"   ‚ùå Falhas:           {global_stats['failed']}")
    print()
    print(f"üìÅ Arquivos salvos em: gs://{DESTINATION_BUCKET_NAME}/{BASE_PATH}/")
    print('‚úÖ Processo conclu√≠do!')
    
    return global_stats, 200


@functions_framework.cloud_event
def crawler_receita_pubsub(cloud_event):
    """
    Handler Pub/Sub - processa arquivo individual, lista arquivos ou processa pasta
    
    Mensagens aceitas:
    1. {"folder": "2024-03", "file": "Empresas0.zip"} - processa arquivo espec√≠fico (RECOMENDADO)
    2. {"folder": "2024-03", "list_files": true} - lista arquivos da pasta
    3. {"folder": "2024-03"} - processa todos arquivos da pasta (pode dar timeout!)
    4. {} ou {"list_folders": true} - lista todas as pastas dispon√≠veis
    """
    try:
        message_data_str = base64.b64decode(cloud_event.data["message"]["data"]).decode("utf-8")
        message_data = json.loads(message_data_str) if message_data_str else {}
    except Exception as e:
        print(f"Erro ao decodificar mensagem: {e}")
        message_data = {}
    
    print('=' * 80)
    print('CRAWLER RECEITA FEDERAL - CLOUD FUNCTION (Pub/Sub)')
    print('=' * 80)
    print(f'Bucket destino: {DESTINATION_BUCKET_NAME}')
    print(f'Caminho base: {BASE_PATH}')
    print()
    
    # CASO 1: Processar arquivo espec√≠fico (RECOMENDADO - evita timeout)
    if 'folder' in message_data and 'file' in message_data:
        folder = message_data['folder']
        file_name = message_data['file']
        
        print(f'üéØ Modo: Processar arquivo individual')
        result = process_single_file(folder, file_name)
        
        print(f'\n{"=" * 80}')
        print('RESULTADO')
        print(f'{"=" * 80}')
        print(f"Pasta: {result['folder']}")
        print(f"Arquivo: {result['file']}")
        print(f"Download: {'‚úÖ' if result['download_success'] else '‚ùå'}")
        print(f"Extra√ß√£o: {'‚úÖ' if result['extraction_success'] else '‚ùå'}")
        print()
        
        if result['download_success'] and result['extraction_success']:
            print('‚úÖ Processamento conclu√≠do com sucesso!')
        else:
            print('‚ùå Processamento falhou!')
        
        return result
    
    # CASO 2: Listar arquivos de uma pasta
    elif 'folder' in message_data and message_data.get('list_files'):
        folder = message_data['folder']
        
        print(f'üìã Modo: Listar arquivos da pasta {folder}')
        files = list_files_in_folder(folder)
        
        print(f'\n{"=" * 80}')
        print(f'ARQUIVOS ENCONTRADOS: {len(files)}')
        print(f'{"=" * 80}')
        for idx, file in enumerate(files, 1):
            print(f'  {idx}. {file}')
        print()
        
        return {'folder': folder, 'files': files, 'count': len(files)}
    
    # CASO 3: Processar pasta inteira (CUIDADO: pode dar timeout!)
    elif 'folder' in message_data:
        folder = message_data['folder']
        if not folder.endswith('/'):
            folder += '/'
        
        print(f'‚ö†Ô∏è  Modo: Processar pasta completa (pode dar timeout em pastas grandes!)')
        print(f'üìÅ Pasta: {folder}')
        
        stats = process_folder(folder)
        
        print(f'\n{"=" * 80}')
        print('RESUMO')
        print(f'{"=" * 80}')
        print(f"Total: {stats['total']}")
        print(f"‚úÖ Extra√≠dos: {stats['extracted']}")
        print(f"‚è≠Ô∏è  Pulados: {stats['skipped']}")
        print(f"‚ùå Falhas: {stats['failed']}")
        print()
        print('‚úÖ Pasta conclu√≠da!')
        
        return stats
    
    # CASO 4: Listar pastas dispon√≠veis
    else:
        print('üìã Modo: Listar pastas dispon√≠veis')
        folders = get_available_folders(BASE_URL)
        
        print(f'\n{"=" * 80}')
        print(f'PASTAS DISPON√çVEIS: {len(folders)}')
        print(f'{"=" * 80}')
        for idx, folder in enumerate(folders, 1):
            print(f'  {idx}. {folder.rstrip("/")}')
        print()
        
        return {'folders': [f.rstrip('/') for f in folders], 'count': len(folders)}
    
    print('Processamento conclu√≠do.')


if __name__ == '__main__':
    # Para testes locais
    class MockRequest:
        pass
    
    crawler_receita_http(MockRequest())

