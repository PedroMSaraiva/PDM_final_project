{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lise de Machine Learning com Spark - Previs√£o de Situa√ß√£o Cadastral (Dataset Silver)\n",
        "\n",
        "Este notebook apresenta uma an√°lise explorat√≥ria completa (EDA) e o processo de treinamento do modelo de ML usando **Apache Spark** para processamento distribu√≠do de dados e prever a situa√ß√£o cadastral de empresas baseado em dados temporais enriquecidos com vari√°veis macroecon√¥micas e dados de empresas.\n",
        "\n",
        "## Objetivo\n",
        "Prever a `situacao_cadastral` de empresas usando dados hist√≥ricos temporais (cnpj + ano_mes) incluindo:\n",
        "- Dados cadastrais (CNAE, UF, natureza jur√≠dica, capital social, porte)\n",
        "- Vari√°veis macroecon√¥micas (SELIC, IPCA, c√¢mbio, desemprego)\n",
        "- Dados de PGFN (d√≠vidas fiscais)\n",
        "- Hist√≥rico temporal da situa√ß√£o cadastral\n",
        "\n",
        "## Diferen√ßas da vers√£o Spark\n",
        "- Usa **PySpark** para processamento distribu√≠do de dados\n",
        "- Processamento escal√°vel para grandes volumes\n",
        "- Modelo final salvo em formato **pickle** para f√°cil deploy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Modelos e m√©tricas (usaremos sklearn para o modelo final)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost n√£o dispon√≠vel. Usando RandomForest.\")\n",
        "\n",
        "# Inicializa Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SituacaoCadastralPrediction\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configura√ß√£o de visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "print(f\"‚úÖ Spark Session criada: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento e Explora√ß√£o Inicial dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carrega os dados usando Spark\n",
        "df_spark = spark.read.csv(\n",
        "    'dataset_metrics_silver.csv',\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Remove coluna de √≠ndice se existir\n",
        "if 'Unnamed: 0' in df_spark.columns:\n",
        "    df_spark = df_spark.drop('Unnamed: 0')\n",
        "\n",
        "# Converte ano_mes para date\n",
        "df_spark = df_spark.withColumn(\n",
        "    'ano_mes',\n",
        "    F.to_date(F.col('ano_mes'), 'yyyy-MM')\n",
        ")\n",
        "\n",
        "# Ordena por cnpj e data\n",
        "df_spark = df_spark.orderBy(['cnpj', 'ano_mes'])\n",
        "\n",
        "# Cache para melhor performance\n",
        "df_spark.cache()\n",
        "\n",
        "# Estat√≠sticas b√°sicas\n",
        "total_registros = df_spark.count()\n",
        "cnpjs_unicos = df_spark.select('cnpj').distinct().count()\n",
        "periodo_min = df_spark.agg(F.min('ano_mes').alias('min_date')).collect()[0]['min_date']\n",
        "periodo_max = df_spark.agg(F.max('ano_mes').alias('max_date')).collect()[0]['max_date']\n",
        "\n",
        "print(f\"üìä Dados carregados: {total_registros:,} registros\")\n",
        "print(f\"üè¢ CNPJs √∫nicos: {cnpjs_unicos:,}\")\n",
        "print(f\"üìÖ Per√≠odo: {periodo_min.strftime('%Y-%m')} a {periodo_max.strftime('%Y-%m')}\")\n",
        "print(f\"\\nColunas dispon√≠veis ({len(df_spark.columns)}):\")\n",
        "print(df_spark.columns)\n",
        "\n",
        "# Mostra primeiras linhas\n",
        "print(f\"\\nPrimeiras linhas:\")\n",
        "df_spark.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informa√ß√µes gerais do dataset\n",
        "print(\"=\"*60)\n",
        "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nShape: ({df_spark.count()}, {len(df_spark.columns)})\")\n",
        "print(f\"\\nTipos de dados:\")\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Valores faltantes\n",
        "print(f\"\\nValores faltantes:\")\n",
        "missing_stats = []\n",
        "for col in df_spark.columns:\n",
        "    missing_count = df_spark.filter(F.col(col).isNull()).count()\n",
        "    if missing_count > 0:\n",
        "        missing_pct = (missing_count / total_registros) * 100\n",
        "        missing_stats.append({\n",
        "            'Coluna': col,\n",
        "            'Faltantes': missing_count,\n",
        "            'Percentual': missing_pct\n",
        "        })\n",
        "\n",
        "if missing_stats:\n",
        "    missing_df = pd.DataFrame(missing_stats).sort_values('Faltantes', ascending=False)\n",
        "    print(missing_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"   Nenhum valor faltante encontrado!\")\n",
        "\n",
        "# Estat√≠sticas descritivas (converte para pandas para visualiza√ß√£o)\n",
        "print(f\"\\nEstat√≠sticas descritivas:\")\n",
        "df_spark.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. An√°lise da Vari√°vel Target (Situa√ß√£o Cadastral)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise da distribui√ß√£o da situa√ß√£o cadastral\n",
        "print(\"=\"*60)\n",
        "print(\"AN√ÅLISE DA VARI√ÅVEL TARGET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Mapeamento das situa√ß√µes cadastrais\n",
        "situacao_map = {\n",
        "    1: 'NULA',\n",
        "    2: 'ATIVA',\n",
        "    3: 'SUSPENSA',\n",
        "    4: 'INAPTA',\n",
        "    8: 'BAIXADA'\n",
        "}\n",
        "\n",
        "# Distribui√ß√£o usando Spark\n",
        "distribuicao_spark = df_spark.groupBy('situacao_cadastral').count().orderBy('situacao_cadastral')\n",
        "distribuicao_pd = distribuicao_spark.toPandas()\n",
        "distribuicao_pd['percentual'] = (distribuicao_pd['count'] / distribuicao_pd['count'].sum()) * 100\n",
        "\n",
        "print(\"\\nüìä Distribui√ß√£o da Situa√ß√£o Cadastral:\")\n",
        "print(\"-\" * 60)\n",
        "for _, row in distribuicao_pd.iterrows():\n",
        "    situacao = int(row['situacao_cadastral'])\n",
        "    count = int(row['count'])\n",
        "    pct = row['percentual']\n",
        "    nome = situacao_map.get(situacao, f'Desconhecida ({situacao})')\n",
        "    print(f\"  {situacao} - {nome:12s}: {count:6,} registros ({pct:5.2f}%)\")\n",
        "\n",
        "# Visualiza√ß√£o (converte para pandas)\n",
        "distribuicao_pd = distribuicao_pd.set_index('situacao_cadastral')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gr√°fico de barras\n",
        "distribuicao_pd['count'].plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
        "axes[0].set_title('Distribui√ß√£o da Situa√ß√£o Cadastral (Contagem)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Situa√ß√£o Cadastral', fontsize=12)\n",
        "axes[0].set_ylabel('Quantidade de Registros', fontsize=12)\n",
        "axes[0].set_xticklabels([f\"{k}\\n{situacao_map.get(k, '?')}\" for k in distribuicao_pd.index], rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Gr√°fico de pizza\n",
        "distribuicao_pd['percentual'].plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90, \n",
        "                      colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc'])\n",
        "axes[1].set_title('Distribui√ß√£o da Situa√ß√£o Cadastral (%)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Observa√ß√£o: Classes desbalanceadas detectadas!\")\n",
        "minority_classes = distribuicao_pd[distribuicao_pd['count'] < 100]\n",
        "if len(minority_classes) > 0:\n",
        "    for situacao, row in minority_classes.iterrows():\n",
        "        nome = situacao_map.get(situacao, f'Desconhecida ({situacao})')\n",
        "        print(f\"   Classe {situacao} ({nome}) tem apenas {int(row['count'])} registros ({row['percentual']:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise temporal da situa√ß√£o cadastral\n",
        "print(\"=\"*60)\n",
        "print(\"AN√ÅLISE TEMPORAL DA SITUA√á√ÉO CADASTRAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Agrupa por ano_mes e situa√ß√£o cadastral usando Spark\n",
        "temp_analysis_spark = df_spark.groupBy(\n",
        "    F.year('ano_mes').alias('ano'),\n",
        "    F.month('ano_mes').alias('mes'),\n",
        "    'situacao_cadastral'\n",
        ").count().orderBy('ano', 'mes', 'situacao_cadastral')\n",
        "\n",
        "# Converte para pandas para visualiza√ß√£o\n",
        "temp_analysis_pd = temp_analysis_spark.toPandas()\n",
        "temp_analysis_pd['ano_mes'] = pd.to_datetime(\n",
        "    temp_analysis_pd['ano'].astype(str) + '-' + temp_analysis_pd['mes'].astype(str).str.zfill(2)\n",
        ")\n",
        "temp_analysis_pd = temp_analysis_pd.pivot_table(\n",
        "    index='ano_mes', \n",
        "    columns='situacao_cadastral', \n",
        "    values='count', \n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Visualiza√ß√£o temporal\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Gr√°fico de linha temporal\n",
        "for col in temp_analysis_pd.columns:\n",
        "    axes[0].plot(temp_analysis_pd.index, temp_analysis_pd[col], \n",
        "                marker='o', label=f\"{col} - {situacao_map.get(col, '?')}\", linewidth=2, markersize=4)\n",
        "axes[0].set_title('Evolu√ß√£o Temporal da Situa√ß√£o Cadastral', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Per√≠odo (Ano-M√™s)', fontsize=12)\n",
        "axes[0].set_ylabel('Quantidade de Registros', fontsize=12)\n",
        "axes[0].legend(loc='best', fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gr√°fico de √°rea empilhada (propor√ß√µes)\n",
        "temp_pct = temp_analysis_pd.div(temp_analysis_pd.sum(axis=1), axis=0) * 100\n",
        "axes[1].stackplot(temp_pct.index, *[temp_pct[col] for col in temp_pct.columns],\n",
        "                  labels=[f\"{col} - {situacao_map.get(col, '?')}\" for col in temp_pct.columns],\n",
        "                  alpha=0.7)\n",
        "axes[1].set_title('Propor√ß√£o Temporal da Situa√ß√£o Cadastral (%)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Per√≠odo (Ano-M√™s)', fontsize=12)\n",
        "axes[1].set_ylabel('Propor√ß√£o (%)', fontsize=12)\n",
        "axes[1].legend(loc='upper left', fontsize=9)\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cria√ß√£o de Features para o Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para criar features temporais usando Spark\n",
        "def create_temporal_features_spark(df):\n",
        "    \"\"\"Cria features temporais a partir de ano_mes usando Spark\"\"\"\n",
        "    df = df.withColumn('ano', F.year('ano_mes'))\n",
        "    df = df.withColumn('mes', F.month('ano_mes'))\n",
        "    df = df.withColumn('trimestre', F.quarter('ano_mes'))\n",
        "    df = df.withColumn('semestre', F.when(F.col('mes') <= 6, 1).otherwise(2))\n",
        "    df = df.withColumn('mes_sin', F.sin(2 * F.pi() * F.col('mes') / 12))\n",
        "    df = df.withColumn('mes_cos', F.cos(2 * F.pi() * F.col('mes') / 12))\n",
        "    \n",
        "    # N√∫mero de meses desde o in√≠cio\n",
        "    min_date = df.agg(F.min('ano_mes').alias('min_date')).collect()[0]['min_date']\n",
        "    df = df.withColumn(\n",
        "        'meses_desde_inicio',\n",
        "        (F.year('ano_mes') - F.year(F.lit(min_date))) * 12 + \n",
        "        (F.month('ano_mes') - F.month(F.lit(min_date)))\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Fun√ß√£o para criar features de lag usando Spark Window Functions\n",
        "def create_lag_features_spark(df, lag_periods=[1, 2, 3, 6, 12]):\n",
        "    \"\"\"Cria features de lag para cada empresa usando Spark\"\"\"\n",
        "    window_spec = Window.partitionBy('cnpj').orderBy('ano_mes')\n",
        "    \n",
        "    for lag in lag_periods:\n",
        "        df = df.withColumn(\n",
        "            f'situacao_cadastral_lag_{lag}',\n",
        "            F.lag('situacao_cadastral', lag).over(window_spec)\n",
        "        )\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Fun√ß√£o para criar features de rolling usando Spark Window Functions\n",
        "def create_rolling_features_spark(df, windows=[3, 6, 12]):\n",
        "    \"\"\"Cria features de rolling statistics por empresa usando Spark\"\"\"\n",
        "    window_spec = Window.partitionBy('cnpj').orderBy('ano_mes')\n",
        "    \n",
        "    for window in windows:\n",
        "        rolling_window = window_spec.rowsBetween(-window + 1, 0)\n",
        "        \n",
        "        df = df.withColumn(\n",
        "            f'situacao_cadastral_rolling_mean_{window}',\n",
        "            F.avg('situacao_cadastral').over(rolling_window)\n",
        "        )\n",
        "        df = df.withColumn(\n",
        "            f'situacao_cadastral_rolling_std_{window}',\n",
        "            F.stddev('situacao_cadastral').over(rolling_window)\n",
        "        )\n",
        "    \n",
        "    # Preenche NaN com 0\n",
        "    for window in windows:\n",
        "        df = df.fillna(0, subset=[f'situacao_cadastral_rolling_mean_{window}',\n",
        "                                  f'situacao_cadastral_rolling_std_{window}'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Fun√ß√£o para criar features agregadas usando Spark\n",
        "def create_aggregated_features_spark(df):\n",
        "    \"\"\"Cria features agregadas por empresa usando Spark\"\"\"\n",
        "    empresa_stats = df.groupBy('cnpj').agg(\n",
        "        F.avg('situacao_cadastral').alias('situacao_cadastral_mean_empresa'),\n",
        "        F.stddev('situacao_cadastral').alias('situacao_cadastral_std_empresa'),\n",
        "        F.min('situacao_cadastral').alias('situacao_cadastral_min_empresa'),\n",
        "        F.max('situacao_cadastral').alias('situacao_cadastral_max_empresa'),\n",
        "        F.count('situacao_cadastral').alias('situacao_cadastral_count_empresa'),\n",
        "        F.first('tempo_atividade_anos').alias('tempo_atividade_anos_first'),\n",
        "        F.first('capital_social').alias('capital_social_first')\n",
        "    )\n",
        "    \n",
        "    df = df.join(empresa_stats, on='cnpj', how='left')\n",
        "    \n",
        "    # Posi√ß√£o temporal\n",
        "    window_spec = Window.partitionBy('cnpj').orderBy('ano_mes')\n",
        "    df = df.withColumn('posicao_temporal', F.row_number().over(window_spec) - 1)\n",
        "    \n",
        "    # Total de registros por empresa\n",
        "    total_registros_empresa = df.groupBy('cnpj').agg(\n",
        "        F.count('*').alias('total_registros_empresa')\n",
        "    )\n",
        "    df = df.join(total_registros_empresa, on='cnpj', how='left')\n",
        "    \n",
        "    # Posi√ß√£o relativa\n",
        "    df = df.withColumn(\n",
        "        'posicao_relativa',\n",
        "        F.col('posicao_temporal') / F.col('total_registros_empresa')\n",
        "    )\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de cria√ß√£o de features definidas!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cria todas as features usando Spark\n",
        "print(\"Criando features temporais...\")\n",
        "df_features = create_temporal_features_spark(df_spark)\n",
        "\n",
        "print(\"Criando features de lag...\")\n",
        "df_features = create_lag_features_spark(df_features)\n",
        "\n",
        "print(\"Criando features de rolling...\")\n",
        "df_features = create_rolling_features_spark(df_features)\n",
        "\n",
        "print(\"Criando features agregadas...\")\n",
        "df_features = create_aggregated_features_spark(df_features)\n",
        "\n",
        "# Remove cache antigo e recache\n",
        "df_spark.unpersist()\n",
        "df_features.cache()\n",
        "\n",
        "print(f\"\\n‚úÖ Features criadas!\")\n",
        "print(f\"   Colunas: {len(df_features.columns)}\")\n",
        "print(f\"   Registros: {df_features.count():,}\")\n",
        "\n",
        "# Mostra algumas das novas features\n",
        "new_features = [col for col in df_features.columns if col not in df_spark.columns]\n",
        "print(f\"\\nüìã Novas features criadas ({len(new_features)}):\")\n",
        "for i, feat in enumerate(new_features[:25], 1):\n",
        "    print(f\"   {i:2d}. {feat}\")\n",
        "if len(new_features) > 25:\n",
        "    print(f\"   ... e mais {len(new_features) - 25} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepara√ß√£o dos Dados para Treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleciona features para o modelo\n",
        "feature_cols = [\n",
        "    # Temporais\n",
        "    'ano', 'mes', 'trimestre', 'semestre', \n",
        "    'mes_sin', 'mes_cos', 'meses_desde_inicio',\n",
        "    \n",
        "    # Categ√≥ricas (ser√£o codificadas)\n",
        "    'cnae_fiscal_principal', 'uf', 'natureza_juridica', 'porte_empresa',\n",
        "    \n",
        "    # Num√©ricas base\n",
        "    'tempo_atividade_anos',\n",
        "    'situacao_cadastral_t_minus_1',\n",
        "    \n",
        "    # Vari√°veis macroecon√¥micas\n",
        "    'selic_meta_mensal_t_minus_1',\n",
        "    'ipca_acumulado_12m_t_minus_1',\n",
        "    'ipca_mensal_t_minus_1',\n",
        "    'cambio_dolar_media_mensal_t_minus_1',\n",
        "    'taxa_desemprego_t_minus_1',\n",
        "    \n",
        "    # Dados de empresa\n",
        "    'capital_social',\n",
        "    \n",
        "    # Dados PGFN\n",
        "    'pgfn_fgts_valor_acumulado_t_minus_1',\n",
        "    'pgfn_naoprev_valor_acumulado_t_minus_1',\n",
        "    'pgfn_prev_valor_acumulado_t_minus_1',\n",
        "    'pgfn_fgts_ajuizados_t_minus_1',\n",
        "    \n",
        "    # Lag features\n",
        "    'situacao_cadastral_lag_1', 'situacao_cadastral_lag_2', \n",
        "    'situacao_cadastral_lag_3', 'situacao_cadastral_lag_6',\n",
        "    \n",
        "    # Rolling features\n",
        "    'situacao_cadastral_rolling_mean_3',\n",
        "    'situacao_cadastral_rolling_mean_6',\n",
        "    'situacao_cadastral_rolling_mean_12',\n",
        "    'situacao_cadastral_rolling_std_3',\n",
        "    'situacao_cadastral_rolling_std_6',\n",
        "    \n",
        "    # Agregadas\n",
        "    'posicao_relativa',\n",
        "    'situacao_cadastral_mean_empresa',\n",
        "    'situacao_cadastral_std_empresa',\n",
        "]\n",
        "\n",
        "# Filtra apenas colunas que existem\n",
        "available_cols = df_features.columns\n",
        "feature_cols = [col for col in feature_cols if col in available_cols]\n",
        "\n",
        "# Remove linhas com NaN nas features selecionadas\n",
        "df_clean = df_features.select(feature_cols + ['situacao_cadastral', 'cnpj', 'ano_mes'])\n",
        "\n",
        "# Remove NaN\n",
        "for col in feature_cols:\n",
        "    df_clean = df_clean.filter(F.col(col).isNotNull())\n",
        "\n",
        "# Recache\n",
        "df_clean.cache()\n",
        "\n",
        "total_clean = df_clean.count()\n",
        "print(f\"üìä Dados ap√≥s limpeza:\")\n",
        "print(f\"   Registros: {total_clean:,}\")\n",
        "print(f\"   Features selecionadas: {len(feature_cols)}\")\n",
        "print(f\"\\nüìã Features selecionadas:\")\n",
        "for i, feat in enumerate(feature_cols, 1):\n",
        "    print(f\"   {i:2d}. {feat}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separa√ß√£o treino/valida√ß√£o/teste (temporal) usando Spark\n",
        "print(\"=\"*60)\n",
        "print(\"SEPARA√á√ÉO TREINO/VALIDA√á√ÉO/TESTE (TEMPORAL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Adiciona um √≠ndice de linha ordenado para fazer a divis√£o temporal\n",
        "# Usa Window Function para criar um √≠ndice sequencial mantendo ordem temporal\n",
        "window_spec = Window.orderBy('ano_mes', 'cnpj')\n",
        "df_clean = df_clean.withColumn('row_index', F.row_number().over(window_spec))\n",
        "\n",
        "# Calcula o total de registros\n",
        "total_count = df_clean.count()\n",
        "split_train_idx = int(total_count * 0.7)\n",
        "split_val_idx = int(total_count * 0.9)\n",
        "\n",
        "print(f\"üìä Total de registros: {total_count:,}\")\n",
        "print(f\"   √çndice de corte treino: {split_train_idx:,} (70%)\")\n",
        "print(f\"   √çndice de corte valida√ß√£o: {split_val_idx:,} (90%)\")\n",
        "\n",
        "# Divide os dados usando Spark\n",
        "df_train = df_clean.filter(F.col('row_index') <= split_train_idx)\n",
        "df_val = df_clean.filter((F.col('row_index') > split_train_idx) & (F.col('row_index') <= split_val_idx))\n",
        "df_test = df_clean.filter(F.col('row_index') > split_val_idx)\n",
        "\n",
        "# Remove a coluna row_index\n",
        "df_train = df_train.drop('row_index')\n",
        "df_val = df_val.drop('row_index')\n",
        "df_test = df_test.drop('row_index')\n",
        "\n",
        "# Cache os DataFrames divididos\n",
        "df_train.cache()\n",
        "df_val.cache()\n",
        "df_test.cache()\n",
        "\n",
        "# Conta os registros\n",
        "train_count = df_train.count()\n",
        "val_count = df_val.count()\n",
        "test_count = df_test.count()\n",
        "\n",
        "print(f\"\\nüìä Divis√£o dos dados:\")\n",
        "print(f\"   Treino:     {train_count:,} registros ({train_count/total_count*100:.1f}%)\")\n",
        "print(f\"   Valida√ß√£o:  {val_count:,} registros ({val_count/total_count*100:.1f}%)\")\n",
        "print(f\"   Teste:      {test_count:,} registros ({test_count/total_count*100:.1f}%)\")\n",
        "\n",
        "# Distribui√ß√£o do target usando Spark\n",
        "print(f\"\\nüìä Distribui√ß√£o do target:\")\n",
        "print(f\"\\n   TREINO:\")\n",
        "train_dist = df_train.groupBy('situacao_cadastral').count().orderBy('situacao_cadastral').show()\n",
        "\n",
        "print(f\"\\n   VALIDA√á√ÉO:\")\n",
        "val_dist = df_val.groupBy('situacao_cadastral').count().orderBy('situacao_cadastral').show()\n",
        "\n",
        "print(f\"\\n   TESTE:\")\n",
        "test_dist = df_test.groupBy('situacao_cadastral').count().orderBy('situacao_cadastral').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codifica features categ√≥ricas usando Spark StringIndexer\n",
        "print(\"=\"*60)\n",
        "print(\"CODIFICA√á√ÉO DE FEATURES CATEG√ìRICAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "categorical_cols = ['cnae_fiscal_principal', 'uf', 'natureza_juridica', 'porte_empresa']\n",
        "categorical_cols = [col for col in categorical_cols if col in feature_cols]\n",
        "\n",
        "# Prepara dados para obter todos os valores √∫nicos (usando union)\n",
        "all_data_for_encoding = df_train.select(categorical_cols).union(\n",
        "    df_val.select(categorical_cols)\n",
        ").union(\n",
        "    df_test.select(categorical_cols)\n",
        ").distinct()\n",
        "\n",
        "# Cria StringIndexers para cada coluna categ√≥rica\n",
        "indexers = {}\n",
        "indexer_models = {}\n",
        "indexed_cols = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    # Pega todos os valores √∫nicos usando Spark\n",
        "    unique_values = all_data_for_encoding.select(col).distinct().orderBy(col).collect()\n",
        "    unique_list = [str(row[col]) for row in unique_values]\n",
        "    \n",
        "    # Cria StringIndexer\n",
        "    indexer = StringIndexer(\n",
        "        inputCol=col,\n",
        "        outputCol=col + '_encoded',\n",
        "        handleInvalid='keep'\n",
        "    )\n",
        "    \n",
        "    indexers[col] = indexer\n",
        "    indexed_cols.append(col + '_encoded')\n",
        "    \n",
        "    print(f\"‚úÖ {col} preparado para codifica√ß√£o: {len(unique_list)} valores √∫nicos\")\n",
        "\n",
        "# Aplica os indexers\n",
        "print(\"\\nüîÑ Aplicando codifica√ß√£o...\")\n",
        "\n",
        "# Treina os indexers usando todos os dados e salva os modelos\n",
        "for col in categorical_cols:\n",
        "    indexer = indexers[col]\n",
        "    # Ajusta usando todos os dados para garantir consist√™ncia\n",
        "    indexer_model = indexer.fit(all_data_for_encoding.select(col))\n",
        "    indexer_models[col] = indexer_model  # Salva o modelo treinado\n",
        "    \n",
        "    # Aplica nos tr√™s conjuntos\n",
        "    df_train = indexer_model.transform(df_train)\n",
        "    df_val = indexer_model.transform(df_val)\n",
        "    df_test = indexer_model.transform(df_test)\n",
        "    \n",
        "    # Remove coluna original\n",
        "    df_train = df_train.drop(col)\n",
        "    df_val = df_val.drop(col)\n",
        "    df_test = df_test.drop(col)\n",
        "\n",
        "# Atualiza feature_cols\n",
        "for col in categorical_cols:\n",
        "    feature_cols = [c for c in feature_cols if c != col] + [col + '_encoded']\n",
        "\n",
        "# Preenche NaN com 0\n",
        "df_train = df_train.fillna(0, subset=feature_cols)\n",
        "df_val = df_val.fillna(0, subset=feature_cols)\n",
        "df_test = df_test.fillna(0, subset=feature_cols)\n",
        "\n",
        "# Seleciona apenas as features necess√°rias\n",
        "df_train_features = df_train.select(feature_cols + ['situacao_cadastral'])\n",
        "df_val_features = df_val.select(feature_cols + ['situacao_cadastral'])\n",
        "df_test_features = df_test.select(feature_cols + ['situacao_cadastral'])\n",
        "\n",
        "# Recache\n",
        "df_train_features.cache()\n",
        "df_val_features.cache()\n",
        "df_test_features.cache()\n",
        "\n",
        "print(f\"\\n‚úÖ Features finais: {len(feature_cols)}\")\n",
        "print(f\"   Treino:     {df_train_features.count():,} registros\")\n",
        "print(f\"   Valida√ß√£o:  {df_val_features.count():,} registros\")\n",
        "print(f\"   Teste:      {df_test_features.count():,} registros\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Treinamento do Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treina modelo Random Forest\n",
        "print(\"=\"*60)\n",
        "print(\"TREINAMENTO DO MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Converte para pandas apenas para o treinamento (sklearn requer pandas/numpy)\n",
        "# Para datasets muito grandes, considere usar Spark MLlib ao inv√©s de sklearn\n",
        "print(\"üîÑ Convertendo dados de treino para pandas...\")\n",
        "train_pd = df_train_features.toPandas()\n",
        "X_train = train_pd[feature_cols].copy()\n",
        "y_train = train_pd['situacao_cadastral'].copy()\n",
        "\n",
        "print(\"üîÑ Convertendo dados de valida√ß√£o para pandas...\")\n",
        "val_pd = df_val_features.toPandas()\n",
        "X_val = val_pd[feature_cols].copy()\n",
        "y_val = val_pd['situacao_cadastral'].copy()\n",
        "\n",
        "print(\"üîÑ Convertendo dados de teste para pandas...\")\n",
        "test_pd = df_test_features.toPandas()\n",
        "X_test = test_pd[feature_cols].copy()\n",
        "y_test = test_pd['situacao_cadastral'].copy()\n",
        "\n",
        "# Libera mem√≥ria do Spark\n",
        "df_train_features.unpersist()\n",
        "df_val_features.unpersist()\n",
        "df_test_features.unpersist()\n",
        "\n",
        "# Calcula pesos das classes para balanceamento\n",
        "class_counts = y_train.value_counts()\n",
        "total = len(y_train)\n",
        "class_weights = {cls: total / (len(class_counts) * count) for cls, count in class_counts.items()}\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  Pesos das classes para balanceamento:\")\n",
        "for cls, weight in sorted(class_counts.items()):\n",
        "    nome = situacao_map.get(cls, f'Desconhecida ({cls})')\n",
        "    print(f\"   Classe {cls} ({nome:12s}): {class_counts[cls]:6,} registros, peso: {class_weights[cls]:.4f}\")\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    class_weight=class_weights,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüîÑ Treinando modelo...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Modelo treinado!\")\n",
        "\n",
        "# Faz previs√µes em valida√ß√£o e teste\n",
        "print(\"\\nüìä Fazendo previs√µes...\")\n",
        "y_pred_val = model.predict(X_val)\n",
        "y_pred_proba_val = model.predict_proba(X_val)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "print(f\"   Previs√µes em valida√ß√£o: {len(y_pred_val)} registros\")\n",
        "print(f\"   Previs√µes em teste:     {len(y_pred)} registros\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Avalia√ß√£o do Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula m√©tricas\n",
        "print(\"=\"*60)\n",
        "print(\"M√âTRICAS DE AVALIA√á√ÉO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# M√©tricas no conjunto de VALIDA√á√ÉO\n",
        "print(f\"\\nüìä M√âTRICAS NO CONJUNTO DE VALIDA√á√ÉO:\")\n",
        "print(\"-\" * 60)\n",
        "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "f1_macro_val = f1_score(y_val, y_pred_val, average='macro')\n",
        "f1_weighted_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "precision_macro_val = precision_score(y_val, y_pred_val, average='macro')\n",
        "recall_macro_val = recall_score(y_val, y_pred_val, average='macro')\n",
        "\n",
        "print(f\"   Acur√°cia:        {accuracy_val:.4f} ({accuracy_val*100:.2f}%)\")\n",
        "print(f\"   F1-Score (macro): {f1_macro_val:.4f}\")\n",
        "print(f\"   F1-Score (weighted): {f1_weighted_val:.4f}\")\n",
        "print(f\"   Precis√£o (macro): {precision_macro_val:.4f}\")\n",
        "print(f\"   Recall (macro):   {recall_macro_val:.4f}\")\n",
        "\n",
        "# M√©tricas no conjunto de TESTE\n",
        "print(f\"\\nüìä M√âTRICAS NO CONJUNTO DE TESTE:\")\n",
        "print(\"-\" * 60)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f\"   Acur√°cia:        {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"   F1-Score (macro): {f1_macro:.4f}\")\n",
        "print(f\"   F1-Score (weighted): {f1_weighted:.4f}\")\n",
        "print(f\"   Precis√£o (macro): {precision_macro:.4f}\")\n",
        "print(f\"   Recall (macro):   {recall_macro:.4f}\")\n",
        "\n",
        "# Classification Report - VALIDA√á√ÉO\n",
        "print(f\"\\nüìã Classification Report - VALIDA√á√ÉO:\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_val, y_pred_val, \n",
        "                            target_names=[f\"{k}-{situacao_map.get(k, '?')}\" for k in sorted(y_val.unique())]))\n",
        "\n",
        "# Classification Report - TESTE\n",
        "print(f\"\\nüìã Classification Report - TESTE:\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[f\"{k}-{situacao_map.get(k, '?')}\" for k in sorted(y_test.unique())]))\n",
        "\n",
        "# Confusion Matrix - VALIDA√á√ÉO\n",
        "cm_val = confusion_matrix(y_val, y_pred_val)\n",
        "print(f\"\\nüìä Matriz de Confus√£o - VALIDA√á√ÉO:\")\n",
        "print(\"-\" * 60)\n",
        "print(cm_val)\n",
        "\n",
        "# Confusion Matrix - TESTE\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"\\nüìä Matriz de Confus√£o - TESTE:\")\n",
        "print(\"-\" * 60)\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o das Matrizes de Confus√£o\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# VALIDA√á√ÉO - Normalizada\n",
        "cm_val_normalized = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_val_normalized, annot=True, fmt='.2%', cmap='Blues', \n",
        "            xticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_val.unique())],\n",
        "            yticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_val.unique())],\n",
        "            ax=axes[0, 0], cbar_kws={'label': 'Propor√ß√£o'})\n",
        "axes[0, 0].set_title('Matriz de Confus√£o Normalizada - VALIDA√á√ÉO', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Predi√ß√£o', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Valor Real', fontsize=12)\n",
        "\n",
        "# VALIDA√á√ÉO - Valores Absolutos\n",
        "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_val.unique())],\n",
        "            yticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_val.unique())],\n",
        "            ax=axes[0, 1], cbar_kws={'label': 'Quantidade'})\n",
        "axes[0, 1].set_title('Matriz de Confus√£o (Valores Absolutos) - VALIDA√á√ÉO', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Predi√ß√£o', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Valor Real', fontsize=12)\n",
        "\n",
        "# TESTE - Normalizada\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', \n",
        "            xticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_test.unique())],\n",
        "            yticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_test.unique())],\n",
        "            ax=axes[1, 0], cbar_kws={'label': 'Propor√ß√£o'})\n",
        "axes[1, 0].set_title('Matriz de Confus√£o Normalizada - TESTE', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Predi√ß√£o', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Valor Real', fontsize=12)\n",
        "\n",
        "# TESTE - Valores Absolutos\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_test.unique())],\n",
        "            yticklabels=[f\"{k}\\n{situacao_map.get(k, '?')}\" for k in sorted(y_test.unique())],\n",
        "            ax=axes[1, 1], cbar_kws={'label': 'Quantidade'})\n",
        "axes[1, 1].set_title('Matriz de Confus√£o (Valores Absolutos) - TESTE', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Predi√ß√£o', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Valor Real', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. An√°lise de Import√¢ncia das Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import√¢ncia das features\n",
        "print(\"=\"*60)\n",
        "print(\"IMPORT√ÇNCIA DAS FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nüìä Top 30 Features Mais Importantes:\")\n",
        "print(\"-\" * 70)\n",
        "for i, row in feature_importance.head(30).iterrows():\n",
        "    print(f\"   {i+1:2d}. {row['feature']:45s}: {row['importance']:.4f}\")\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "top_n = 30\n",
        "top_features = feature_importance.head(top_n)\n",
        "\n",
        "ax.barh(range(len(top_features)), top_features['importance'], color='steelblue', edgecolor='black')\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'], fontsize=10)\n",
        "ax.set_xlabel('Import√¢ncia', fontsize=12)\n",
        "ax.set_title(f'Top {top_n} Features Mais Importantes', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lise por categoria de features\n",
        "print(f\"\\nüìä Import√¢ncia por Categoria de Features:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "categories = {\n",
        "    'Temporais': ['ano', 'mes', 'trimestre', 'semestre', 'mes_sin', 'mes_cos', 'meses_desde_inicio'],\n",
        "    'Categ√≥ricas': [c for c in feature_cols if '_encoded' in c],\n",
        "    'Macroecon√¥micas': ['selic_meta_mensal_t_minus_1', 'ipca_acumulado_12m_t_minus_1', \n",
        "                        'ipca_mensal_t_minus_1', 'cambio_dolar_media_mensal_t_minus_1', \n",
        "                        'taxa_desemprego_t_minus_1'],\n",
        "    'PGFN': ['pgfn_fgts_valor_acumulado_t_minus_1', 'pgfn_naoprev_valor_acumulado_t_minus_1',\n",
        "             'pgfn_prev_valor_acumulado_t_minus_1', 'pgfn_fgts_ajuizados_t_minus_1'],\n",
        "    'Lag': [c for c in feature_cols if 'lag' in c],\n",
        "    'Rolling': [c for c in feature_cols if 'rolling' in c],\n",
        "    'Empresa': ['tempo_atividade_anos', 'capital_social', 'situacao_cadastral_t_minus_1'],\n",
        "    'Agregadas': [c for c in feature_cols if 'empresa' in c or 'posicao' in c]\n",
        "}\n",
        "\n",
        "for category, features in categories.items():\n",
        "    cat_features = [f for f in features if f in feature_importance['feature'].values]\n",
        "    if cat_features:\n",
        "        cat_importance = feature_importance[feature_importance['feature'].isin(cat_features)]['importance'].sum()\n",
        "        print(f\"   {category:20s}: {cat_importance:.4f} ({len(cat_features)} features)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Salvamento do Modelo em Pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salva o modelo e os indexers em pickle\n",
        "print(\"=\"*60)\n",
        "print(\"SALVAMENTO DO MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cria diret√≥rio para modelos se n√£o existir\n",
        "model_dir = 'models_pickle'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Salva o modelo\n",
        "model_path = os.path.join(model_dir, 'random_forest_model.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "print(f\"‚úÖ Modelo salvo em: {model_path}\")\n",
        "\n",
        "# Salva os mapeamentos dos indexers do Spark\n",
        "# Nota: StringIndexer models do Spark n√£o s√£o diretamente serializ√°veis com pickle\n",
        "# Vamos salvar um mapeamento dos valores √∫nicos para recriar os indexers\n",
        "indexers_path = os.path.join(model_dir, 'spark_indexers_mapping.pkl')\n",
        "indexers_mapping = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    # Obt√©m os valores √∫nicos e seus √≠ndices do modelo treinado\n",
        "    # O StringIndexer ordena por frequ√™ncia, ent√£o precisamos obter o mapeamento correto\n",
        "    unique_values = all_data_for_encoding.select(col).distinct().orderBy(col).collect()\n",
        "    unique_list = [str(row[col]) for row in unique_values]\n",
        "    # Cria um mapeamento valor -> √≠ndice (o StringIndexer usa ordem de frequ√™ncia, mas salvamos ordem alfab√©tica)\n",
        "    indexers_mapping[col] = {val: idx for idx, val in enumerate(unique_list)}\n",
        "\n",
        "with open(indexers_path, 'wb') as f:\n",
        "    pickle.dump(indexers_mapping, f)\n",
        "print(f\"‚úÖ Mapeamento dos indexers do Spark salvo em: {indexers_path}\")\n",
        "\n",
        "# Salva a lista de features\n",
        "features_path = os.path.join(model_dir, 'feature_columns.pkl')\n",
        "with open(features_path, 'wb') as f:\n",
        "    pickle.dump(feature_cols, f)\n",
        "print(f\"‚úÖ Lista de features salva em: {features_path}\")\n",
        "\n",
        "# Salva informa√ß√µes do modelo\n",
        "model_info = {\n",
        "    'accuracy_val': accuracy_val,\n",
        "    'accuracy_test': accuracy,\n",
        "    'f1_macro_val': f1_macro_val,\n",
        "    'f1_macro_test': f1_macro,\n",
        "    'f1_weighted_val': f1_weighted_val,\n",
        "    'f1_weighted_test': f1_weighted,\n",
        "    'n_features': len(feature_cols),\n",
        "    'n_train': len(X_train),\n",
        "    'n_val': len(X_val),\n",
        "    'n_test': len(X_test),\n",
        "    'model_type': 'RandomForestClassifier',\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': 15,\n",
        "    'categorical_cols': categorical_cols\n",
        "}\n",
        "\n",
        "info_path = os.path.join(model_dir, 'model_info.pkl')\n",
        "with open(info_path, 'wb') as f:\n",
        "    pickle.dump(model_info, f)\n",
        "print(f\"‚úÖ Informa√ß√µes do modelo salvas em: {info_path}\")\n",
        "\n",
        "print(f\"\\nüì¶ Todos os arquivos salvos no diret√≥rio: {model_dir}/\")\n",
        "print(f\"   - random_forest_model.pkl\")\n",
        "print(f\"   - spark_indexers_mapping.pkl\")\n",
        "print(f\"   - feature_columns.pkl\")\n",
        "print(f\"   - model_info.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Exemplo de Carregamento e Uso do Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de como carregar e usar o modelo salvo\n",
        "print(\"=\"*60)\n",
        "print(\"EXEMPLO DE CARREGAMENTO DO MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Carrega o modelo\n",
        "with open(model_path, 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# Carrega os mapeamentos dos indexers\n",
        "with open(indexers_path, 'rb') as f:\n",
        "    loaded_indexers_mapping = pickle.load(f)\n",
        "\n",
        "# Carrega as features\n",
        "with open(features_path, 'rb') as f:\n",
        "    loaded_features = pickle.load(f)\n",
        "\n",
        "print(\"‚úÖ Modelo e componentes carregados com sucesso!\")\n",
        "\n",
        "# Exemplo de predi√ß√£o com dados de teste\n",
        "print(f\"\\nüìä Testando predi√ß√£o com {len(X_test)} amostras do conjunto de teste...\")\n",
        "sample_predictions = loaded_model.predict(X_test.head(10))\n",
        "sample_proba = loaded_model.predict_proba(X_test.head(10))\n",
        "\n",
        "print(f\"\\n   Primeiras 10 predi√ß√µes:\")\n",
        "for i, (pred, proba) in enumerate(zip(sample_predictions, sample_proba)):\n",
        "    proba_max = proba.max()\n",
        "    situacao_nome = situacao_map.get(pred, f'Desconhecida ({pred})')\n",
        "    print(f\"   Amostra {i+1}: Predi√ß√£o = {pred} ({situacao_nome}), Confian√ßa = {proba_max:.4f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo funcionando corretamente!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumo Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo final\n",
        "print(\"=\"*80)\n",
        "print(\"                    RESUMO FINAL DA AN√ÅLISE                     \")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"   Total de registros: {total_registros:,}\")\n",
        "print(f\"   CNPJs √∫nicos: {cnpjs_unicos:,}\")\n",
        "print(f\"   Per√≠odo: {periodo_min.strftime('%Y-%m')} a {periodo_max.strftime('%Y-%m')}\")\n",
        "print(f\"   Registros ap√≥s limpeza: {total_clean:,}\")\n",
        "\n",
        "print(f\"\\nüìä Divis√£o dos Dados:\")\n",
        "print(f\"   Treino:     {len(X_train):,} registros (70.0%)\")\n",
        "print(f\"   Valida√ß√£o:  {len(X_val):,} registros (20.0%)\")\n",
        "print(f\"   Teste:      {len(X_test):,} registros (10.0%)\")\n",
        "\n",
        "print(f\"\\nüîß Features:\")\n",
        "# Calcula n√∫mero de features criadas (colunas adicionais)\n",
        "n_features_created = len(df_features.columns) - 19  # 19 √© o n√∫mero original de colunas do dataset\n",
        "print(f\"   Features criadas: {n_features_created}\")\n",
        "print(f\"   Features finais utilizadas: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\nü§ñ Modelo:\")\n",
        "print(f\"   Algoritmo: Random Forest\")\n",
        "print(f\"   Processamento: Apache Spark\")\n",
        "print(f\"   Formato de salvamento: Pickle\")\n",
        "print(f\"\\n   M√©tricas - VALIDA√á√ÉO:\")\n",
        "print(f\"      Acur√°cia:        {accuracy_val:.4f} ({accuracy_val*100:.2f}%)\")\n",
        "print(f\"      F1-Score (macro): {f1_macro_val:.4f}\")\n",
        "print(f\"      F1-Score (weighted): {f1_weighted_val:.4f}\")\n",
        "print(f\"\\n   M√©tricas - TESTE:\")\n",
        "print(f\"      Acur√°cia:        {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"      F1-Score (macro): {f1_macro:.4f}\")\n",
        "print(f\"      F1-Score (weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà Top 5 Features Mais Importantes:\")\n",
        "for i, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"   {i+1}. {row['feature']:45s}: {row['importance']:.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Modelo Salvo:\")\n",
        "print(f\"   Diret√≥rio: {model_dir}/\")\n",
        "print(f\"   Arquivos: random_forest_model.pkl, spark_indexers_mapping.pkl, feature_columns.pkl, model_info.pkl\")\n",
        "\n",
        "print(f\"\\n‚úÖ An√°lise conclu√≠da!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encerra a Spark Session\n",
        "spark.stop()\n",
        "print(\"‚úÖ Spark Session encerrada.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
